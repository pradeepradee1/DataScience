{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Introduction to the Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/keras/keras_tuner\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/keras/keras_tuner.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The Keras Tuner is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called *hyperparameter tuning* or *hypertuning*.\n",
    "\n",
    "Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n",
    "1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n",
    "2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier\n",
    "\n",
    "In this tutorial, you will use the Keras Tuner to perform hypertuning for an image classification application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g83Lwsy-Aq2_"
   },
   "source": [
    "Install and import the Keras Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpMLpbt9jcO6",
    "outputId": "b2849590-fc8a-4020-d5bb-9ed512c6ae73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m112.6/128.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_leAIdFKAxAD"
   },
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReV_UXOgCZvx"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HljH_ENLEdHa"
   },
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OHlHs9Wj_PUM",
    "outputId": "6228fb14-cfc7-4a79-cadd-a507aced0a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 1us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ASBvAdfu6Z7f",
    "outputId": "70dfadee-7fc5-4e70-82d1-23acfa047b09"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OE2BwT9d6tI9",
    "outputId": "18ed96bf-0e53-4586-9bd2-cfb52f929964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
       "          1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
       "          0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
       "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
       "         10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
       "         72,  15],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
       "        172,  66],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
       "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
       "        229,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
       "        173,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
       "        202,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
       "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
       "        209,  52],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
       "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
       "        167,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0],\n",
       "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
       "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
       "         77,   0],\n",
       "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
       "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
       "        159,   0],\n",
       "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
       "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
       "        215,   0],\n",
       "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
       "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
       "        246,   0],\n",
       "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
       "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
       "        225,   0],\n",
       "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
       "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
       "        229,  29],\n",
       "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
       "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
       "        230,  67],\n",
       "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
       "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
       "        206, 115],\n",
       "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
       "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
       "        210,  92],\n",
       "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
       "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
       "        170,   0],\n",
       "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
       "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "rRK5BhMf7j8P",
    "outputId": "280c50eb-f511-4921-d9bd-328f33c1670a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img_train[0],cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "gIdwNRN88Yda",
    "outputId": "abfbaaa5-e5c9-4059-e044-56c3c0f98e58"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img_train[1],cmap='binary')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pWDtx9Zt6XjM",
    "outputId": "1fe00360-c4db-4adf-a2fc-59ed28e66aa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bLVhXs3xrUD0"
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X4XMOCF4M0yv",
    "outputId": "e1039d3b-35af-440b-ea41-ab886d230ecd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.5378702e-05, 0.0000000e+00, 0.0000000e+00, 1.9992310e-04,\n",
       "        1.1226452e-03, 0.0000000e+00, 0.0000000e+00, 1.5378702e-05,\n",
       "        6.1514809e-05, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.5378702e-05, 1.5378702e-05, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.6136101e-05, 0.0000000e+00, 5.5363326e-04, 2.0915035e-03,\n",
       "        1.9530950e-03, 9.5347944e-04, 8.3044986e-04, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.5378702e-05, 4.6136101e-05,\n",
       "        6.1514809e-05, 0.0000000e+00, 0.0000000e+00, 4.6136101e-05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        9.2272203e-05, 0.0000000e+00, 1.5686274e-03, 3.1372549e-03,\n",
       "        2.7066513e-03, 2.0607461e-03, 2.2145330e-03, 1.8915802e-03,\n",
       "        3.5371011e-04, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.8454441e-04, 1.5378701e-04, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 2.3836987e-03, 3.6293734e-03,\n",
       "        3.1833909e-03, 2.7374087e-03, 1.6455210e-03, 2.3990774e-03,\n",
       "        2.4759709e-03, 1.6762784e-03, 9.8423695e-04, 3.5371011e-04,\n",
       "        1.1841600e-03, 1.9992313e-03, 1.1072665e-03, 2.3068051e-04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.5378702e-05,\n",
       "        0.0000000e+00, 1.0611304e-03, 3.1833909e-03, 3.4294503e-03,\n",
       "        3.3525568e-03, 3.3217994e-03, 3.3217994e-03, 2.5067283e-03,\n",
       "        1.9530950e-03, 1.8608228e-03, 1.8762015e-03, 2.2452904e-03,\n",
       "        2.1683970e-03, 1.3533257e-03, 2.6451366e-03, 1.0149943e-03],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.5378702e-05, 1.5378702e-05, 1.5378702e-05,\n",
       "        0.0000000e+00, 3.0757401e-03, 3.5678586e-03, 3.5678586e-03,\n",
       "        3.5832373e-03, 3.5217225e-03, 3.4294503e-03, 3.4294503e-03,\n",
       "        3.3064208e-03, 3.2756634e-03, 2.5221070e-03, 1.9530950e-03,\n",
       "        1.8915802e-03, 3.0142253e-03, 3.5217225e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 2.8143022e-03, 3.4602077e-03, 3.3217994e-03,\n",
       "        3.4294503e-03, 3.5063438e-03, 3.6139947e-03, 3.4909651e-03,\n",
       "        3.4448290e-03, 3.4140716e-03, 3.4448290e-03, 3.3986929e-03,\n",
       "        3.4294503e-03, 3.7677817e-03, 2.6605153e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 2.9680892e-03, 3.5063438e-03, 3.3525568e-03,\n",
       "        3.2756634e-03, 3.0449827e-03, 2.7681661e-03, 3.2602844e-03,\n",
       "        3.2295270e-03, 3.2449057e-03, 3.2756634e-03, 3.4294503e-03,\n",
       "        3.3833142e-03, 3.7370243e-03, 3.1064975e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 1.5378702e-05, 4.6136101e-05, 0.0000000e+00,\n",
       "        1.8454441e-04, 3.3679355e-03, 3.3833142e-03, 3.2602844e-03,\n",
       "        3.3525568e-03, 2.9527105e-03, 2.5990005e-03, 3.4909651e-03,\n",
       "        3.1987696e-03, 3.3525568e-03, 3.4448290e-03, 3.2602844e-03,\n",
       "        3.4755864e-03, 3.0296040e-03, 3.2141483e-03, 7.9969241e-04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 9.2272203e-05, 0.0000000e+00,\n",
       "        1.5224913e-03, 3.7524030e-03, 3.4140716e-03, 3.3833142e-03,\n",
       "        3.3525568e-03, 3.1218762e-03, 3.0449827e-03, 3.3986929e-03,\n",
       "        3.3064208e-03, 3.2756634e-03, 3.4140716e-03, 3.3833142e-03,\n",
       "        3.7677817e-03, 1.8300654e-03, 2.5682431e-03, 8.6120726e-04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 6.1514809e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        8.4582856e-04, 3.6293734e-03, 3.5063438e-03, 3.5371012e-03,\n",
       "        3.5063438e-03, 3.6908882e-03, 3.5678586e-03, 3.2756634e-03,\n",
       "        3.3525568e-03, 3.4294503e-03, 3.5986160e-03, 3.3371781e-03,\n",
       "        3.3371781e-03, 3.2141483e-03, 1.4148405e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.5378702e-05, 6.1514809e-05,\n",
       "        9.2272203e-05, 1.0765091e-04, 3.0757405e-05, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        3.6447521e-03, 3.4755864e-03, 3.3371781e-03, 3.4294503e-03,\n",
       "        3.4140716e-03, 3.3679355e-03, 3.4140716e-03, 3.3986929e-03,\n",
       "        3.3217994e-03, 3.4294503e-03, 3.5217225e-03, 3.3064208e-03,\n",
       "        3.3525568e-03, 3.9215689e-03, 1.1841600e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 4.6136101e-05, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 9.5347944e-04, 2.2299117e-03, 3.1372549e-03,\n",
       "        3.5063438e-03, 3.1833909e-03, 3.2756634e-03, 3.3986929e-03,\n",
       "        3.3525568e-03, 3.1987696e-03, 3.2449057e-03, 3.3525568e-03,\n",
       "        3.4448290e-03, 3.4294503e-03, 3.3679355e-03, 3.3064208e-03,\n",
       "        3.4448290e-03, 3.7524030e-03, 2.4452135e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        2.7681663e-04, 6.7666284e-04, 1.2610535e-03, 1.6455210e-03,\n",
       "        2.9065744e-03, 3.5063438e-03, 3.3833142e-03, 3.4140716e-03,\n",
       "        3.3371781e-03, 3.4755864e-03, 3.0757401e-03, 3.1526336e-03,\n",
       "        3.2449057e-03, 3.5371012e-03, 3.4448290e-03, 3.5986160e-03,\n",
       "        2.7066513e-03, 2.8911957e-03, 3.8446751e-03, 3.8139177e-03,\n",
       "        3.5832373e-03, 3.6601308e-03, 3.3064208e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 8.7658595e-04, 2.8758170e-03, 3.1987696e-03,\n",
       "        3.4448290e-03, 3.3986929e-03, 3.4448290e-03, 3.1987696e-03,\n",
       "        3.1372549e-03, 3.2910421e-03, 3.1987696e-03, 3.2141483e-03,\n",
       "        3.0757401e-03, 2.4452135e-03, 3.7677817e-03, 2.9680892e-03,\n",
       "        3.1680122e-03, 3.4294503e-03, 3.9215689e-03, 3.9215689e-03,\n",
       "        3.3986929e-03, 3.5986160e-03, 3.3986929e-03, 3.2449057e-03,\n",
       "        3.3833142e-03, 3.5678586e-03, 3.7831604e-03, 0.0000000e+00],\n",
       "       [4.6136101e-05, 3.1064975e-03, 3.5063438e-03, 3.4448290e-03,\n",
       "        3.3986929e-03, 3.2449057e-03, 3.2449057e-03, 3.2910421e-03,\n",
       "        3.1526336e-03, 3.1526336e-03, 3.1526336e-03, 3.3833142e-03,\n",
       "        3.6908882e-03, 1.2302961e-03, 2.3068052e-03, 3.9215689e-03,\n",
       "        3.5217225e-03, 3.3986929e-03, 2.8911957e-03, 2.3683200e-03,\n",
       "        2.9373318e-03, 3.2295270e-03, 3.1372549e-03, 3.2141483e-03,\n",
       "        3.4140716e-03, 3.5063438e-03, 3.4602077e-03, 0.0000000e+00],\n",
       "       [1.5071126e-03, 3.5832373e-03, 3.0449827e-03, 3.2295270e-03,\n",
       "        3.4140716e-03, 3.5217225e-03, 3.5217225e-03, 3.5986160e-03,\n",
       "        3.8292964e-03, 3.3833142e-03, 2.9834679e-03, 3.3064208e-03,\n",
       "        3.3371781e-03, 3.7062669e-03, 9.9961564e-04, 1.1226452e-03,\n",
       "        1.6301422e-03, 1.7993080e-03, 2.5836218e-03, 3.3679355e-03,\n",
       "        3.3986929e-03, 3.3064208e-03, 3.3371781e-03, 3.4294503e-03,\n",
       "        3.4294503e-03, 3.4448290e-03, 3.5217225e-03, 4.4598232e-04],\n",
       "       [1.1534026e-03, 3.1372549e-03, 3.2602844e-03, 3.1372549e-03,\n",
       "        2.9680892e-03, 3.1526336e-03, 3.2449057e-03, 3.4602077e-03,\n",
       "        3.3217994e-03, 2.8450596e-03, 3.0296040e-03, 3.1680122e-03,\n",
       "        3.0449827e-03, 3.2756634e-03, 3.6908882e-03, 2.9988466e-03,\n",
       "        3.4909651e-03, 3.7677817e-03, 3.6755095e-03, 3.4294503e-03,\n",
       "        3.3525568e-03, 3.2602844e-03, 3.2141483e-03, 3.4140716e-03,\n",
       "        3.3833142e-03, 3.3986929e-03, 3.5371012e-03, 1.0303730e-03],\n",
       "       [7.3817762e-04, 3.1218762e-03, 2.8143022e-03, 2.9834679e-03,\n",
       "        3.2756634e-03, 3.0296040e-03, 2.8450596e-03, 2.9219531e-03,\n",
       "        2.9834679e-03, 2.9527105e-03, 3.1064975e-03, 3.2910421e-03,\n",
       "        3.3679355e-03, 3.3986929e-03, 3.3833142e-03, 3.6293734e-03,\n",
       "        3.4602077e-03, 3.3217994e-03, 3.0603614e-03, 3.1680122e-03,\n",
       "        2.8604383e-03, 2.7835448e-03, 2.7220300e-03, 2.6451366e-03,\n",
       "        2.7835448e-03, 3.1526336e-03, 3.1680122e-03, 1.7685506e-03],\n",
       "       [0.0000000e+00, 1.8762015e-03, 3.3679355e-03, 2.9680892e-03,\n",
       "        2.7527874e-03, 2.6297579e-03, 2.8143022e-03, 3.0142253e-03,\n",
       "        3.1372549e-03, 3.2295270e-03, 3.2756634e-03, 3.1833909e-03,\n",
       "        3.2449057e-03, 3.2295270e-03, 3.0757401e-03, 3.0142253e-03,\n",
       "        2.9834679e-03, 2.9373318e-03, 2.9988466e-03, 2.9373318e-03,\n",
       "        3.0449827e-03, 2.9527105e-03, 2.7066513e-03, 2.3990774e-03,\n",
       "        2.5682431e-03, 2.7220300e-03, 3.2295270e-03, 1.4148405e-03],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.1380239e-03, 2.9065744e-03,\n",
       "        3.2602844e-03, 2.9373318e-03, 2.6912726e-03, 2.6451366e-03,\n",
       "        2.6912726e-03, 2.7835448e-03, 2.8450596e-03, 2.8911957e-03,\n",
       "        2.9065744e-03, 2.8911957e-03, 2.9680892e-03, 3.0449827e-03,\n",
       "        3.1372549e-03, 3.2141483e-03, 3.2295270e-03, 3.2295270e-03,\n",
       "        3.2449057e-03, 2.8911957e-03, 2.8911957e-03, 2.9834679e-03,\n",
       "        2.9527105e-03, 3.3217994e-03, 2.6143792e-03, 0.0000000e+00],\n",
       "       [3.0757405e-05, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0149943e-03, 3.0757401e-03, 3.4140716e-03, 3.6447521e-03,\n",
       "        3.6755095e-03, 3.7216456e-03, 3.7831604e-03, 3.7370243e-03,\n",
       "        3.7524030e-03, 3.3986929e-03, 3.3833142e-03, 2.9680892e-03,\n",
       "        2.9373318e-03, 2.7527874e-03, 2.7989235e-03, 2.7989235e-03,\n",
       "        2.7835448e-03, 2.7066513e-03, 2.5528644e-03, 2.5836218e-03,\n",
       "        1.5224913e-03, 8.9196465e-04, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.1514805e-04,\n",
       "        9.3810074e-04, 6.7666284e-04, 1.1072665e-03, 6.3052675e-04,\n",
       "        5.3825456e-04, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00],\n",
       "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5YEL2H2Ax3e"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a *hypermodel*.\n",
    "\n",
    "You can define a hypermodel through two approaches:\n",
    "\n",
    "* By using a model builder function\n",
    "* By subclassing the `HyperModel` class of the Keras Tuner API\n",
    "\n",
    "You can also use two pre-defined [HyperModel](https://keras.io/api/keras_tuner/hypermodels/) classes - [HyperXception](https://keras.io/api/keras_tuner/hypermodels/hyper_xception/) and [HyperResNet](https://keras.io/api/keras_tuner/hypermodels/hyper_resnet/) for computer vision applications.\n",
    "\n",
    "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZQKodC-jtsva"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  # Sequential Layer\n",
    "  model = keras.Sequential()\n",
    "  # Input Layers\n",
    "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "\n",
    "  #Note :\n",
    "  # 1) hp is hyper patameter technique\n",
    "  # 2) choosing the optimal neurons between minimum 32 and maximum 512 for hidden layers\n",
    "  # 3) layer name is units\n",
    "\n",
    "  # Hidden layer\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "\n",
    "  #Since we have multi classification we have to create 10 neauron in OP layer\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01 or 0.001 or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J1VYw4q3x0b"
   },
   "source": [
    "## Instantiate the tuner and perform hypertuning\n",
    "\n",
    "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, you use the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) tuner.\n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oichQFly6Y46"
   },
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaIhhdKf9VtI"
   },
   "source": [
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwhBdXx0Ekj8"
   },
   "source": [
    "Create a callback to stop training early after reaching a certain value for the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "WT9IkS9NEjLc"
   },
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKghEo15Tduy"
   },
   "source": [
    "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dSBQcTHF9cKt",
    "outputId": "857234aa-0ad2-4f99-87bc-76ac40ab417d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 01m 23s]\n",
      "val_accuracy: 0.8555833101272583\n",
      "\n",
      "Best val_accuracy So Far: 0.8870000243186951\n",
      "Total elapsed time: 00h 15m 32s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 416 and the optimal learning rate for the optimizer\n",
      "is 0.01.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.search(img_train, label_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lak_ylf88xBv"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "McO82AXOuxXh",
    "outputId": "c51c5150-9b4f-4070-b44e-6b2465f986d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1500/1500 [==============================] - 6s 3ms/step - loss: 0.5939 - accuracy: 0.7856 - val_loss: 0.4702 - val_accuracy: 0.8289\n",
      "Epoch 2/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.4240 - accuracy: 0.8450 - val_loss: 0.4333 - val_accuracy: 0.8467\n",
      "Epoch 3/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3847 - accuracy: 0.8597 - val_loss: 0.3903 - val_accuracy: 0.8581\n",
      "Epoch 4/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3591 - accuracy: 0.8685 - val_loss: 0.3902 - val_accuracy: 0.8516\n",
      "Epoch 5/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3387 - accuracy: 0.8748 - val_loss: 0.3367 - val_accuracy: 0.8798\n",
      "Epoch 6/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3223 - accuracy: 0.8799 - val_loss: 0.3540 - val_accuracy: 0.8754\n",
      "Epoch 7/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.3115 - accuracy: 0.8836 - val_loss: 0.3407 - val_accuracy: 0.8772\n",
      "Epoch 8/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2984 - accuracy: 0.8885 - val_loss: 0.3532 - val_accuracy: 0.8739\n",
      "Epoch 9/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2904 - accuracy: 0.8919 - val_loss: 0.3359 - val_accuracy: 0.8766\n",
      "Epoch 10/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2813 - accuracy: 0.8939 - val_loss: 0.3386 - val_accuracy: 0.8792\n",
      "Epoch 11/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2714 - accuracy: 0.8978 - val_loss: 0.3346 - val_accuracy: 0.8803\n",
      "Epoch 12/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2654 - accuracy: 0.8999 - val_loss: 0.3226 - val_accuracy: 0.8842\n",
      "Epoch 13/50\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.2583 - accuracy: 0.9028 - val_loss: 0.3156 - val_accuracy: 0.8888\n",
      "Epoch 14/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2506 - accuracy: 0.9052 - val_loss: 0.3410 - val_accuracy: 0.8807\n",
      "Epoch 15/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2450 - accuracy: 0.9070 - val_loss: 0.3116 - val_accuracy: 0.8900\n",
      "Epoch 16/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2392 - accuracy: 0.9095 - val_loss: 0.3130 - val_accuracy: 0.8898\n",
      "Epoch 17/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2315 - accuracy: 0.9128 - val_loss: 0.3588 - val_accuracy: 0.8773\n",
      "Epoch 18/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2262 - accuracy: 0.9140 - val_loss: 0.3321 - val_accuracy: 0.8843\n",
      "Epoch 19/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2236 - accuracy: 0.9162 - val_loss: 0.3115 - val_accuracy: 0.8893\n",
      "Epoch 20/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2171 - accuracy: 0.9176 - val_loss: 0.3327 - val_accuracy: 0.8822\n",
      "Epoch 21/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2133 - accuracy: 0.9186 - val_loss: 0.3228 - val_accuracy: 0.8920\n",
      "Epoch 22/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2066 - accuracy: 0.9213 - val_loss: 0.3482 - val_accuracy: 0.8860\n",
      "Epoch 23/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2040 - accuracy: 0.9227 - val_loss: 0.3094 - val_accuracy: 0.8942\n",
      "Epoch 24/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2015 - accuracy: 0.9242 - val_loss: 0.3400 - val_accuracy: 0.8898\n",
      "Epoch 25/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1965 - accuracy: 0.9252 - val_loss: 0.3579 - val_accuracy: 0.8794\n",
      "Epoch 26/50\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1926 - accuracy: 0.9265 - val_loss: 0.3170 - val_accuracy: 0.8935\n",
      "Epoch 27/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1862 - accuracy: 0.9295 - val_loss: 0.3364 - val_accuracy: 0.8913\n",
      "Epoch 28/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1865 - accuracy: 0.9290 - val_loss: 0.3512 - val_accuracy: 0.8867\n",
      "Epoch 29/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1831 - accuracy: 0.9304 - val_loss: 0.3368 - val_accuracy: 0.8916\n",
      "Epoch 30/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1769 - accuracy: 0.9331 - val_loss: 0.3600 - val_accuracy: 0.8868\n",
      "Epoch 31/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1762 - accuracy: 0.9323 - val_loss: 0.3435 - val_accuracy: 0.8925\n",
      "Epoch 32/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1695 - accuracy: 0.9361 - val_loss: 0.3478 - val_accuracy: 0.8915\n",
      "Epoch 33/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1689 - accuracy: 0.9357 - val_loss: 0.3416 - val_accuracy: 0.8933\n",
      "Epoch 34/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1652 - accuracy: 0.9372 - val_loss: 0.3541 - val_accuracy: 0.8894\n",
      "Epoch 35/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1626 - accuracy: 0.9373 - val_loss: 0.3472 - val_accuracy: 0.8932\n",
      "Epoch 36/50\n",
      "1500/1500 [==============================] - 7s 4ms/step - loss: 0.1614 - accuracy: 0.9382 - val_loss: 0.3487 - val_accuracy: 0.8913\n",
      "Epoch 37/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1563 - accuracy: 0.9414 - val_loss: 0.3677 - val_accuracy: 0.8916\n",
      "Epoch 38/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1548 - accuracy: 0.9415 - val_loss: 0.3546 - val_accuracy: 0.8911\n",
      "Epoch 39/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1552 - accuracy: 0.9413 - val_loss: 0.3475 - val_accuracy: 0.8917\n",
      "Epoch 40/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1512 - accuracy: 0.9424 - val_loss: 0.3645 - val_accuracy: 0.8954\n",
      "Epoch 41/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1467 - accuracy: 0.9438 - val_loss: 0.4209 - val_accuracy: 0.8836\n",
      "Epoch 42/50\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1432 - accuracy: 0.9454 - val_loss: 0.3660 - val_accuracy: 0.8932\n",
      "Epoch 43/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1431 - accuracy: 0.9457 - val_loss: 0.3822 - val_accuracy: 0.8878\n",
      "Epoch 44/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1394 - accuracy: 0.9471 - val_loss: 0.3787 - val_accuracy: 0.8845\n",
      "Epoch 45/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1373 - accuracy: 0.9481 - val_loss: 0.3820 - val_accuracy: 0.8878\n",
      "Epoch 46/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1348 - accuracy: 0.9484 - val_loss: 0.3796 - val_accuracy: 0.8879\n",
      "Epoch 47/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1329 - accuracy: 0.9491 - val_loss: 0.3837 - val_accuracy: 0.8950\n",
      "Epoch 48/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1310 - accuracy: 0.9495 - val_loss: 0.4036 - val_accuracy: 0.8882\n",
      "Epoch 49/50\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1283 - accuracy: 0.9518 - val_loss: 0.4168 - val_accuracy: 0.8906\n",
      "Epoch 50/50\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1280 - accuracy: 0.9508 - val_loss: 0.3993 - val_accuracy: 0.8903\n",
      "Best epoch: 40\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(img_train, label_train, epochs=50, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOTSirSTI3Gp"
   },
   "source": [
    "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NoiPUEHmMhCe",
    "outputId": "a23df69d-bbe5-4f05-b490-f88a582c3a6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.5912 - accuracy: 0.7831 - val_loss: 0.4464 - val_accuracy: 0.8382\n",
      "Epoch 2/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.4210 - accuracy: 0.8466 - val_loss: 0.4088 - val_accuracy: 0.8537\n",
      "Epoch 3/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3804 - accuracy: 0.8608 - val_loss: 0.3717 - val_accuracy: 0.8637\n",
      "Epoch 4/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3554 - accuracy: 0.8681 - val_loss: 0.3575 - val_accuracy: 0.8698\n",
      "Epoch 5/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.3361 - accuracy: 0.8755 - val_loss: 0.3598 - val_accuracy: 0.8696\n",
      "Epoch 6/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3217 - accuracy: 0.8804 - val_loss: 0.3509 - val_accuracy: 0.8770\n",
      "Epoch 7/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.3087 - accuracy: 0.8847 - val_loss: 0.3300 - val_accuracy: 0.8809\n",
      "Epoch 8/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2966 - accuracy: 0.8889 - val_loss: 0.3588 - val_accuracy: 0.8712\n",
      "Epoch 9/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2882 - accuracy: 0.8926 - val_loss: 0.3461 - val_accuracy: 0.8755\n",
      "Epoch 10/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2798 - accuracy: 0.8952 - val_loss: 0.3362 - val_accuracy: 0.8769\n",
      "Epoch 11/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2703 - accuracy: 0.8983 - val_loss: 0.3327 - val_accuracy: 0.8774\n",
      "Epoch 12/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2638 - accuracy: 0.9007 - val_loss: 0.3406 - val_accuracy: 0.8782\n",
      "Epoch 13/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2563 - accuracy: 0.9041 - val_loss: 0.3348 - val_accuracy: 0.8804\n",
      "Epoch 14/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2506 - accuracy: 0.9043 - val_loss: 0.3273 - val_accuracy: 0.8824\n",
      "Epoch 15/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2450 - accuracy: 0.9083 - val_loss: 0.3378 - val_accuracy: 0.8815\n",
      "Epoch 16/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2397 - accuracy: 0.9092 - val_loss: 0.3256 - val_accuracy: 0.8842\n",
      "Epoch 17/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2325 - accuracy: 0.9128 - val_loss: 0.3264 - val_accuracy: 0.8871\n",
      "Epoch 18/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2297 - accuracy: 0.9130 - val_loss: 0.3159 - val_accuracy: 0.8895\n",
      "Epoch 19/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2208 - accuracy: 0.9162 - val_loss: 0.3289 - val_accuracy: 0.8870\n",
      "Epoch 20/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2163 - accuracy: 0.9191 - val_loss: 0.3190 - val_accuracy: 0.8913\n",
      "Epoch 21/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2103 - accuracy: 0.9205 - val_loss: 0.3303 - val_accuracy: 0.8889\n",
      "Epoch 22/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.2077 - accuracy: 0.9219 - val_loss: 0.3266 - val_accuracy: 0.8863\n",
      "Epoch 23/40\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.2034 - accuracy: 0.9233 - val_loss: 0.3297 - val_accuracy: 0.8891\n",
      "Epoch 24/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.2001 - accuracy: 0.9239 - val_loss: 0.3252 - val_accuracy: 0.8888\n",
      "Epoch 25/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1974 - accuracy: 0.9260 - val_loss: 0.3449 - val_accuracy: 0.8847\n",
      "Epoch 26/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1907 - accuracy: 0.9266 - val_loss: 0.3336 - val_accuracy: 0.8882\n",
      "Epoch 27/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1885 - accuracy: 0.9282 - val_loss: 0.3281 - val_accuracy: 0.8923\n",
      "Epoch 28/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1872 - accuracy: 0.9295 - val_loss: 0.3297 - val_accuracy: 0.8925\n",
      "Epoch 29/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1814 - accuracy: 0.9305 - val_loss: 0.3532 - val_accuracy: 0.8843\n",
      "Epoch 30/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1779 - accuracy: 0.9316 - val_loss: 0.3655 - val_accuracy: 0.8831\n",
      "Epoch 31/40\n",
      "1500/1500 [==============================] - 5s 4ms/step - loss: 0.1749 - accuracy: 0.9325 - val_loss: 0.3432 - val_accuracy: 0.8897\n",
      "Epoch 32/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1730 - accuracy: 0.9350 - val_loss: 0.3441 - val_accuracy: 0.8894\n",
      "Epoch 33/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1698 - accuracy: 0.9362 - val_loss: 0.3533 - val_accuracy: 0.8893\n",
      "Epoch 34/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1635 - accuracy: 0.9379 - val_loss: 0.3331 - val_accuracy: 0.8912\n",
      "Epoch 35/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1639 - accuracy: 0.9378 - val_loss: 0.3469 - val_accuracy: 0.8939\n",
      "Epoch 36/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1613 - accuracy: 0.9386 - val_loss: 0.3623 - val_accuracy: 0.8896\n",
      "Epoch 37/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1542 - accuracy: 0.9426 - val_loss: 0.3646 - val_accuracy: 0.8857\n",
      "Epoch 38/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1533 - accuracy: 0.9414 - val_loss: 0.3676 - val_accuracy: 0.8878\n",
      "Epoch 39/40\n",
      "1500/1500 [==============================] - 6s 4ms/step - loss: 0.1503 - accuracy: 0.9433 - val_loss: 0.3600 - val_accuracy: 0.8928\n",
      "Epoch 40/40\n",
      "1500/1500 [==============================] - 5s 3ms/step - loss: 0.1486 - accuracy: 0.9433 - val_loss: 0.3882 - val_accuracy: 0.8792\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7a6d8b4bfbb0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqU5ZVAaag2v"
   },
   "source": [
    "To finish this tutorial, evaluate the hypermodel on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E0BTp9Ealjb",
    "outputId": "8db6df5b-4ff7-4510-db9b-d56c7dab7137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.4229 - accuracy: 0.8741\n",
      "[test loss, test accuracy]: [0.4229118525981903, 0.8741000294685364]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(img_test, label_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQRpPHZsz-eC"
   },
   "source": [
    "The `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKwLOzKpFGAj"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to use the Keras Tuner to tune hyperparameters for a model. To learn more about the Keras Tuner, check out these additional resources:\n",
    "\n",
    "* [Keras Tuner on the TensorFlow blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
    "* [Keras Tuner website](https://keras-team.github.io/keras-tuner/)\n",
    "\n",
    "Also check out the [HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) in TensorBoard to interactively tune your model hyperparameters."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
